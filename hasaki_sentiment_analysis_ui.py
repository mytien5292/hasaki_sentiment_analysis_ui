import numpy as np
import pandas as pd
import streamlit as st
import json
import time


from hasaki_sentiment_analysis_prediction import predict_sentiment
from hasaki_sentiment_analysis_visualization import analyze_and_visualize
from streamlit_searchbox import st_searchbox

# ======= Load data part =======
@st.cache_data
def load_boost_words():
    with open('data/tools/boost_words.txt', 'r', encoding='utf-8') as file:
        boost_words = [line.strip() for line in file]
    return boost_words

@st.cache_data
def load_data_products():
    data = pd.read_csv('data/san_pham_processed.csv')

    if "data_feedbacks" not in st.session_state:
        st.session_state.data_feedbacks = load_data_feedbacks()

    data_feedbacks = st.session_state.data_feedbacks

    data['so_luong_danh_gia'] = data['ma_san_pham'].map(data_feedbacks['ma_san_pham'].value_counts()).fillna(0).astype(int)

    data['ten_san_pham_sl_danh_gia'] = data['ten_san_pham'] + " (" + data['so_luong_danh_gia'].astype(str) + " ƒë√°nh gi√°)"
    data['ma_san_pham_sl_danh_gia'] = data['ma_san_pham'].astype(str) + " (" + data['so_luong_danh_gia'].astype(str) + " ƒë√°nh gi√°)"

    return data

@st.cache_data
def load_product_mapping():
    if "data_products" not in st.session_state:
        st.session_state.data_products = load_data_products()
    
    data = st.session_state.data_products

    product_mapping = dict(zip(data['ten_san_pham_sl_danh_gia'], data['ma_san_pham']))
    return product_mapping

def is_existed(addded_words, word):
    for x in addded_words:
        if word in x:
            return True
    return False

def apply_boost_words(text, boost_words):
    parts = text.split()
    added_words = []

    for i in range(5, 0, -1):
        for j in range(len(parts) - i + 1):
            sub_text = ' '.join(parts[j:j+i])
            if sub_text in boost_words and not is_existed(added_words, sub_text):
                added_words.append(sub_text)

    for word in added_words:
        num_boost = word.count(' ')
        text = text.replace(word, word.replace(" ", "_"))
        for i in range(num_boost):
            text = text + " " + word.replace(" ", "_")

    return text

@st.cache_data
def load_data_feedbacks():
    data = pd.read_csv('data/Danh_gia_with_label.csv')
    boost_words = load_boost_words()

    data['normalized_text_with_boost_words'] = data['normalized_text'].apply(lambda x: apply_boost_words(x, boost_words))

    return data

# ======= Logic part =======
def search_product_name(product_name):
    if "data_products" not in st.session_state:
        st.session_state.data_products = load_data_products()
    data_products = st.session_state.data_products
    filter_rules = data_products['ten_san_pham_sl_danh_gia'].str.contains(product_name, case=False)
    product = data_products[filter_rules]

    search_all_text = "T√¨m t·∫•t c·∫£ s·∫£n ph·∫©m c√≥ ch·ª©a t·ª´ kh√≥a " + product_name
    result = [search_all_text] + list(product["ten_san_pham_sl_danh_gia"].values)

    return result

def search_product_code(product_code):
    if "data_products" not in st.session_state:
        st.session_state.data_products = load_data_products()
    
    data_products = st.session_state.data_products
    filter_rules = data_products['ma_san_pham_sl_danh_gia'].astype(str).str.contains(product_code, case=False)
    product = data_products[filter_rules]

    search_all_text = "T√¨m t·∫•t c·∫£ s·∫£n ph·∫©m c√≥ ch·ª©a t·ª´ kh√≥a " + product_code
    result = [search_all_text] + list(product["ma_san_pham_sl_danh_gia"].values)

    return result

def get_product_info(product_id):
    if "data_products" not in st.session_state:
        st.session_state.data_products = load_data_products()

    if "data_feedbacks" not in st.session_state:
        st.session_state.data_feedbacks = load_data_feedbacks()
    
    data_products = st.session_state.data_products
    data_feedbacks = st.session_state.data_feedbacks

    product_feedbacks = data_feedbacks[data_feedbacks['ma_san_pham'] == product_id]
    product_info = data_products[data_products['ma_san_pham'] == product_id]

    return product_info, product_feedbacks

# ======= Analysis part =======


# ======= UI part =======
def show_product_info(product_id):
    product_infos, product_feedbacks = get_product_info(product_id)

    if product_infos.empty:
        st.write("Kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m.")
        return

    for product_info in product_infos.itertuples():
        #st.write(f"""##### {product_info.ten_san_pham}\n""")
        st.markdown(
            f"""
            <h5 style='color: green;'>{product_info.ten_san_pham}</h5>
            """,
            unsafe_allow_html=True,
        )
        st.image(product_info.hinh_san_pham, width=400)
        st.write(f"""[Xem chi ti·∫øt s·∫£n ph·∫©m]({product_info.link_san_pham})""")
        
        formatted_price = f"{product_info.gia_ban:,}ƒë"
        st.markdown(
            f"""
            **M√£ s·∫£n ph·∫©m**: {product_info.ma_san_pham}  
            **Gi√° b√°n**: <span style='color: red;'>{formatted_price}</span>

            **ƒêi·ªÉm trung b√¨nh**: {product_info.diem_trung_binh} ‚≠ê
            """,
            unsafe_allow_html=True,
        )
 
    st.markdown(
        """
        <div style='background-color: #66BB6A; padding: 10px; border-radius: 5px; text-align: center;'>
            <h2 style='color: white; margin: 0;'>Ph√¢n t√≠ch s·∫£n ph·∫©m</h2>
        </div>
        """,
        unsafe_allow_html=True,
    )
    #st.image("media/sentiment_distribution.png", use_container_width=True, width=200)
    # Th√™m kho·∫£ng c√°ch tr∆∞·ªõc h√†ng ·∫£nh
    st.markdown("<br>", unsafe_allow_html=True)

    analyze_and_visualize(product_infos, product_feedbacks)

def business_objective_content():
    #st.image("media/hasaki_banner.jpg", width=800)

    #st.subheader("ƒê·∫∑t v·∫•n ƒë·ªÅ")
    # Ti√™u ƒë·ªÅ ch√≠nh v√† c√°c ti√™u ƒë·ªÅ ph·ª• v·ªõi m√†u xanh l·ª•c
    st.markdown(
        """
        <h3 style='color: green;'>1. Gi·ªõi thi·ªáu b√†i to√°n Sentiment Analysis s·∫£n ph·∫©m m·ªπ ph·∫©m cho Hasaki</h3>
        """,
        unsafe_allow_html=True,
    )

    # N·ªôi dung d∆∞·ªõi ti√™u ƒë·ªÅ ch√≠nh
    st.write("""
    Hasaki.vn l√† m·ªôt l√† m·ªôt n·ªÅn t·∫£ng b√°n l·∫ª m·ªπ ph·∫©m tr·ª±c tuy·∫øn l·ªõn, c√≥ h√†ng ng√†n s·∫£n ph·∫©m v√† ƒë√°nh gi√° t·ª´ kh√°ch h√†ng. C√°c ƒë√°nh gi√° (reviews) n√†y ch·ª©a th√¥ng tin quan tr·ªçng, l√† ch√¨a kho√° gi√∫p Hasaki hi·ªÉu ƒë∆∞·ª£c c·∫£m nh·∫≠n c·ªßa kh√°ch h√†ng v·ªÅ s·∫£n ph·∫©m, d·ªãch v·ª• v√† tr·∫£i nghi·ªám mua s·∫Øm. Tuy nhi√™n, vi·ªác ph√¢n t√≠ch th·ªß c√¥ng r·∫•t t·ªën th·ªùi gian v√† kh√≥ th·ª±c hi·ªán ·ªü quy m√¥ l·ªõn. V√¨ v·∫≠y c·∫ßn ph·∫£i x√¢y d·ª±ng h·ªá th·ªëng ph√¢n t√≠ch c·∫£m x√∫c (Sentiment Analysis) ƒë·ªÉ t·ª± ƒë·ªông h√≥a vi·ªác ph√¢n lo·∫°i v√† tr√≠ch xu·∫•t th√¥ng tin t·ª´ ƒë√°nh gi√° c·ªßa kh√°ch h√†ng.
    """)

    # Ti√™u ƒë·ªÅ "2. M·ª•c ti√™u c·ªßa h·ªá th·ªëng Sentiment Analysis:" v·ªõi m√†u xanh l·ª•c
    st.markdown(
        """
        <h3 style='color: green;'>2. M·ª•c ti√™u c·ªßa h·ªá th·ªëng Sentiment Analysis</h3>
        """,
        unsafe_allow_html=True,
    )

    # N·ªôi dung d∆∞·ªõi ti√™u ƒë·ªÅ ph·ª•
    st.write("""
    - **Ph√¢n lo·∫°i c·∫£m x√∫c c·ªßa c√°c b√¨nh lu·∫≠n**: X√°c ƒë·ªãnh c·∫£m x√∫c c·ªßa kh√°ch h√†ng trong c√°c ƒë√°nh gi√° s·∫£n ph·∫©m l√† t√≠ch c·ª±c, ti√™u c·ª±c.
    - **Hi·ªÉu s√¢u h∆°n v·ªÅ kh√°ch h√†ng**: Tr√≠ch xu·∫•t th√¥ng tin h·ªØu √≠ch nh∆∞: kh√°ch h√†ng th√≠ch ƒëi·ªÅu g√¨ v√† kh√¥ng h√†i l√≤ng ƒëi·ªÅu g√¨.
    - **H·ªó tr·ª£ ra quy·∫øt ƒë·ªãnh**: Gi√∫p Hasaki.vn c·∫£i thi·ªán s·∫£n ph·∫©m v√† d·ªãch v·ª•, tƒÉng m·ª©c ƒë·ªô h√†i l√≤ng v√† gi·ªØ ch√¢n kh√°ch h√†ng.
    - **·ª®ng d·ª•ng th·ª±c t·∫ø**: T·ª± ƒë·ªông g·∫Øn nh√£n ƒë√°nh gi√° tr√™n website. H·ªó tr·ª£ cho h·ªá th·ªëng chƒÉm s√≥c kh√°ch h√†ng v√† n√¢ng cao ch·∫•t l∆∞·ª£ng marketing.
    """)

    # Ti√™u ƒë·ªÅ "3. L·ª£i √≠ch cho Hasaki:" v·ªõi m√†u xanh l·ª•c
    st.markdown(
        """
        <h3 style='color: green;'>3. L·ª£i √≠ch cho Hasaki</h3>
        """,
        unsafe_allow_html=True,
    )

    # N·ªôi dung d∆∞·ªõi ti√™u ƒë·ªÅ ph·ª•
    st.write("""
    - C·∫£i thi·ªán tr·∫£i nghi·ªám kh√°ch h√†ng.
    - T·ªëi ∆∞u h√≥a chi·∫øn l∆∞·ª£c kinh doanh.
    - Kh·∫≥ng ƒë·ªãnh v·ªã th·∫ø d·∫´n ƒë·∫ßu trong ng√†nh m·ªπ ph·∫©m t·∫°i Vi·ªát Nam.
    """)

def build_project_construction():
    #st.image("media/hasaki_banner.jpg", width=800)
    #1. Crawl th√™m d·ªØ li·ªáu t·ª´ c√°c categorical
    st.markdown(
        """
        <h3 style='color: green;'>1. Crawl th√™m d·ªØ li·ªáu t·ª´ c√°c categorical</h3>
        """,
        unsafe_allow_html=True,
    )
    st.markdown(
        """
        Crawl th√™m d·ªØ li·ªáu t·ª´ c√°c categorical tr√™n trang web Hasaki.vn: chƒÉm s√≥c c∆° th·ªÉ, chƒÉm s√≥c t√≥c v√† da ƒë·∫ßu, trang ƒëi·ªÉm, chƒÉm s√≥c c√° nh√¢n, chƒÉm s√≥c da m·∫∑t.
        """,
        unsafe_allow_html=True,
    )
    st.image("media/crawl_du_lieu.png", use_container_width=True, width=800)

    #2. Labeling data 
    st.markdown(
        """
        <h3 style='color: green;'>2. Labeling data</h3>
        """,
        unsafe_allow_html=True,
    )
    st.markdown(
        """
        **C·∫ßn 2 l·ªõp labels:**
        - 2 s·∫Øc th√°i c·ªßa b√¨nh lu·∫≠n: Positive/Negative (Kh√¥ng c√≥ neutral do kh√≥ nh·∫≠n d·∫°ng lo·∫°i n√†y)
        - 4 ch·ªß ƒë·ªÅ (topics) m√† c√°c b√¨nh lu·∫≠n hay nh·∫Øc ƒë·∫øn:
            + Pricing - Gi√° c·∫£
            + Fragrance - M√πi th∆°m
            + Usage Experience - Tr·∫£i nghi·ªám s·ª≠ d·ª•ng
            + Body Impact - T√°c ƒë·ªông th·∫ø n√†o l√™n c∆° th·ªÉ
        """,
        unsafe_allow_html=True,
    )

    st.markdown(
        """
        **V·ªõi label s·∫Øc th√°i c·ªßa b√¨nh lu·∫≠n:**
        - N·∫øu review c√≥ s·ªë sao >= 4 => Positive
        - N·∫øu review c√≥ s·ªë sao < 4 => Negative

        V·ªõi label v·ªÅ ch·ªß ƒë·ªÅ m√† b√¨nh lu·∫≠n ƒëang n√≥i ƒë·∫øn => D√πng matching t·ª´ kho√°.
        """,
        unsafe_allow_html=True,
    )
    #3. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
    st.markdown(
        """
        <h3 style='color: green;'>3. Ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu</h3>
        """,
        unsafe_allow_html=True,
    )

    st.markdown(
        """
        **C√°c b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu:**
        1. B·ªè c√°c b√¨nh lu·∫≠n b·ªã duplicate ho·∫∑c nan
        2. B·ªè c√°c d·∫•u space, kho·∫£ng tr·∫Øng d∆∞ th·ª´a
        3. Thay th·∫ø k√≠ t·ª± emoji
        4. Thay th·∫ø c√°c t·ª´ ti·∫øng Anh th√†nh ti·∫øng Vi·ªát
        5. Thay th·∫ø c√°c t·ª´ teencode th√†nh t·ª´ ƒë·ªçc ƒë∆∞·ª£c
        6. B·ªè c√°c stopword
        """,
        unsafe_allow_html=True,
    )

    #4. Ph√¢n t√≠ch d·ªØ li·ªáu
    st.markdown(
        """
        <h3 style='color: green;'>4. Ph√¢n t√≠ch d·ªØ li·ªáu</h3>
        """,
        unsafe_allow_html=True,
    )
    
    st.markdown(
    """
    <h4>Tr·ª±c quan ho√° d·ªØ li·ªáu sau khi ph√¢n t√≠ch b·∫±ng c√°c bi·ªÉu ƒë·ªì</h4>
    """,
    unsafe_allow_html=True,
    )
    
    st.markdown(
    """
    **T·ª∑ l·ªá b√¨nh lu·∫≠n t√≠ch c·ª±c v√† ti√™u c·ª±c**
    """,
    unsafe_allow_html=True,
    )
    st.image("media/ti_le_pos_neg.jpg", use_container_width=True, width=350)

    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown(
    """
    **T·ª∑ l·ªá Topics trong b√¨nh lu·∫≠n t√≠ch c·ª±c**
    """,
    unsafe_allow_html=True,
    )
    
    st.image("media/sentiment_label_positive.png", use_container_width=True, width=200)

    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown(
    """
    **T·ª∑ l·ªá Topics trong b√¨nh lu·∫≠n ti√™u c·ª±c**
    """,
    unsafe_allow_html=True,
    )
    
    st.image("media/sentimet_label_negative.png", use_container_width=True, width=200)
    
    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown(
    """
    **Ph√¢n b·ªë s·ªë l∆∞·ª£ng b√¨nh lu·∫≠n tr√™n m·ªói s·∫£n ph·∫©m**
    """,
    unsafe_allow_html=True,
    )
    
    st.image("media/so_luong_binh_luan_tren_moi_user.png", use_container_width=True, width=800)
    
    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown(
    """
    **S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n theo th√°ng v√† nƒÉm**
    """,
    unsafe_allow_html=True,
    )
    st.image("media/so_luong_binh_luan_theo_thang_va_nam.png", use_container_width=True, width=800)

    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown(
    """
    **S·ªë l∆∞·ª£ng b√¨nh lu·∫≠n theo gi·ªù**
    """,
    unsafe_allow_html=True,
    )
    st.image("media/so_luong_binh_luan_theo_gio.png", use_container_width=True, width=800)

    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown(
    """
    **Bi·ªÉu ƒë·ªì t·∫ßn su·∫•t s·ªë sao**
    """,
    unsafe_allow_html=True,
    )
    st.image("media/tan_suat_so_sao.png", use_container_width=True, width=800)

    st.markdown(
    """
    **WordCloud cho c√°c sentiment labels**
    """,
    unsafe_allow_html=True,
    )
    # T·∫°o hai c·ªôt
    col1, col2 = st.columns(2)

    # Hi·ªÉn th·ªã h√¨nh ·∫£nh trong c·ªôt ƒë·∫ßu ti√™n
    with col1:
        st.image("media/positive_comment.png", use_container_width=True)

    # Hi·ªÉn th·ªã h√¨nh ·∫£nh trong c·ªôt th·ª© hai
    with col2:
        st.image("media/negative_comment.png", use_container_width=True)

    st.markdown("<br>", unsafe_allow_html=True)
    #5. X√¢y d·ª±ng model
    st.markdown(
        """
        <h3 style='color: green;'>5 X√¢y d·ª±ng model</h3>
        """,
        unsafe_allow_html=True,
    )
    
    st.markdown(
    """
    <h4>5.1 X√¢y d·ª±ng model b·∫±ng Scikit-Learn</h4>
    """,
    unsafe_allow_html=True,
    )

    st.markdown(
        """
        - S·ª≠ d·ª•ng TF-IDF ƒë·ªÉ vectorize n·ªôi dung b√¨nh lu·∫≠n
        - Build model b·∫±ng c√°c thu·∫≠t to√°n sau v√† k·∫øt qu·∫£
        """,
        unsafe_allow_html=True,
    )
    st.markdown("<br>", unsafe_allow_html=True)
    st.image("media/scikit_learn.png", use_container_width=True)

    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown(
    """
    <h4>5.2 X√¢y d·ª±ng model b·∫±ng PySpark</h4>
    """,
    unsafe_allow_html=True,
    )

    st.markdown(
        """
        - S·ª≠ d·ª•ng TF-IDF ƒë·ªÉ vectorize n·ªôi dung b√¨nh lu·∫≠n
        - Build model b·∫±ng c√°c thu·∫≠t to√°n sau v√† k·∫øt qu·∫£
        """,
        unsafe_allow_html=True,
    )
    st.markdown("<br>", unsafe_allow_html=True)
    st.image("media/PySpark.png", use_container_width=True)

    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown(
    """
    <h4>5.3 X√¢y d·ª±ng model b·∫±ng th∆∞ vi·ªán kh√°c</h4>
    """,
    unsafe_allow_html=True,
    )
    st.markdown(
        """
        - S·ª≠ d·ª•ng SentenceTransformer ƒë·ªÉ vectorize n·ªôi dung b√¨nh lu·∫≠n
        - Build model b·∫±ng c√°c thu·∫≠t to√°n
        """,
        unsafe_allow_html=True,
    )
    st.markdown("<br>", unsafe_allow_html=True)
    st.image("media/thu_vien_khac.png", use_container_width=True)
    st.markdown("<br>", unsafe_allow_html=True)


    #6. K·∫øt lu·∫≠n
    st.markdown(
        """
        <h3 style='color: green;'>6 K·∫øt lu·∫≠n</h3>
        """,
        unsafe_allow_html=True,
    )
    st.markdown(
        """
    S·ª≠ d·ª•ng thu·∫≠t to√°n Random Forest ƒë·ªÉ √°p d·ª•ng ph√¢n lo·∫°i c·∫£m x√∫c c·ªßa b√¨nh lu·∫≠n v√¨ ƒë·ªô ch√≠nh x√°c cao.
        """,
        unsafe_allow_html=True,
    )
    st.markdown(
        """
    N√¢ng cao hi·ªáu su·∫•t cho m√¥ h√¨nh.
        """,
        unsafe_allow_html=True,
    )
    st.markdown("<br>", unsafe_allow_html=True)
    st.image("media/nang_cao_hieu_suat_random.png", use_container_width=True)

##

def build_product_analysis():
    if "product_mapping" not in st.session_state:
        st.session_state.product_mapping = load_product_mapping()
    
    product_mapping = st.session_state.product_mapping

    search_type = st.radio("Ch·ªçn c√°ch t√¨m ki·∫øm s·∫£n ph·∫©m:", ("T√¨m ki·∫øm theo t√™n s·∫£n ph·∫©m", "T√¨m ki·∫øm theo m√£ s·∫£n ph·∫©m"))

    if search_type == "T√¨m ki·∫øm theo t√™n s·∫£n ph·∫©m":
        selected_value = st_searchbox(
            search_product_name,
            placeholder="T√¨m ki·∫øm t√™n s·∫£n ph·∫©m",
            default_use_searchterm=True,
        )

        print("--------------------------------")
        print(selected_value)
        print("--------------------------------")

        selected_value = product_mapping.get(selected_value, "Kh√¥ng t√¨m th·∫•y m√£ s·∫£n ph·∫©m")
    else:
        selected_value = st_searchbox(
            search_product_code,
            placeholder="T√¨m ki·∫øm m√£ s·∫£n ph·∫©m",
            default_use_searchterm=True,
        )

        print("--------------------------------")
        print(selected_value)
        print("--------------------------------")

        if selected_value is not None:
            selected_value = int(selected_value.split(" ")[0])
    
    show_product_info(selected_value)
   
def new_product_analysis():
    input_type = st.radio("Ch·ªçn c√°ch nh·∫≠p d·ªØ li·ªáu:", ("Nh·∫≠p t·ª´ b√†n ph√≠m", "Nh·∫≠p t·ª´ file"))

    input_feedbacks = []

    if input_type == "Nh·∫≠p t·ª´ b√†n ph√≠m":
        feedback_content = st.text_area("N·ªôi dung b√¨nh lu·∫≠n", height=200)
        st.write("""V√≠ d·ª•:\n\n- sp r·∫•t ok\n\n- M√πi h∆∞∆°ng kh√¥ng ƒë∆∞·ª£c th∆°m.\n\n- M√πi h·∫Øc c·ªìn, b√¥i v√†o da th·∫•y r√°t, c·∫£m gi√°c s∆∞ng s∆∞ng ·ªü v√πng b√¥i n√°ch sau 2 ng√†y bong v·∫£y.\n\n- Ti·∫øc l√† kh√¥ng ch·ªãu mua em n√≥ s·ªõm h∆°n. M√¨nh m·ªõi d√πng 1 l·∫ßn sau khi t·∫Øm, qua h√¥m sau th·∫•y kh√¥ tho√°ng m√† h·∫øt m√πi h·∫≥n. ƒê√∫ng ch√¢n √°i. Ch·∫Øc ph·∫£i mua 1-2 l·ªç tr·ªØ s·∫µn\n\n
                 """)
        input_feedbacks = feedback_content.split('\n')
    else:
        uploaded_file = st.file_uploader("Ch·ªçn file d·ªØ li·ªáu m·ªõi (csv ho·∫∑c txt)", type=["csv", "txt"], accept_multiple_files=False)

        if uploaded_file is None:
            st.write("V√≠ d·ª• file d·ªØ li·ªáu csv:")
            example_input_csv = pd.read_csv("data/input_file_example.csv")
            st.write(example_input_csv.head())

            st.write("V√≠ d·ª• file d·ªØ li·ªáu txt:")
            example_input_txt = pd.read_csv("data/input_file_example.txt", sep="\t", header=None)
            st.write(example_input_txt)

        if uploaded_file is not None:
            st.write("N·ªôi dung d·ªØ li·ªáu v·ª´a t·∫£i l√™n:")

            if uploaded_file.type == "text/plain":
                input_feedbacks = uploaded_file.read().decode("utf-8").splitlines()
                for feedback in input_feedbacks:
                    st.write(feedback)
            else:
                data = pd.read_csv(uploaded_file)
                st.write(data.head())
                input_feedbacks = data["noi_dung_binh_luan"].tolist()
    
    input_feedbacks = [feedback for feedback in input_feedbacks if feedback.strip()]

    if st.button("Ph√¢n t√≠ch d·ªØ li·ªáu"):
        st.write("K·∫øt qu·∫£ ph√¢n t√≠ch d·ªØ li·ªáu:")
        result = predict_sentiment(input_feedbacks)
        st.write(result)

        st.download_button(
            label="Download k·∫øt qu·∫£ (.csv)",
            data=result.to_csv(index=False),
            file_name="sentiment_result.csv",
            mime="text/csv",
        )
            

# ======= Main content =======
def main_content():
    # Ti√™u ƒë·ªÅ v·ªõi m√†u xanh l·ª•c
    # ƒê·∫∑t c·∫•u h√¨nh trang r·ªông h∆°n
    st.set_page_config(
        #page_title="My App",  # Ti√™u ƒë·ªÅ c·ªßa ·ª©ng d·ª•ng
        #page_icon="üåü",       # Bi·ªÉu t∆∞·ª£ng hi·ªÉn th·ªã tr√™n tab
        layout="wide",        # Ch·∫ø ƒë·ªô hi·ªÉn th·ªã: "wide" ho·∫∑c "centered"
    )
    
# Hi·ªÉn th·ªã ti√™u ƒë·ªÅ v·ªõi m√†u ch·ªØ tr·∫Øng v√† khung n·ªÅn xanh l√°
    # Hi·ªÉn th·ªã ti√™u ƒë·ªÅ v·ªõi khung n·ªÅn
    st.image("media/tieu_de.png", use_container_width=True)

    st.markdown("<br>", unsafe_allow_html=True)
    st.markdown("<br>", unsafe_allow_html=True)
    #st.subheader("Th·ª±c hi·ªán d·ª± √°n")
       # Ti√™u ƒë·ªÅ Menu
    st.sidebar.markdown(
        """
        <div style='color: green; font-size: 18px; font-weight: bold; margin-bottom: -20px;'>
            Menu
        </div>
        """,
        unsafe_allow_html=True,
    )
    # Menu ch√≠nh
    menu = ["M·ª•c ti√™u d·ª± √°n", "Th·ª±c hi·ªán d·ª± √°n", "Ph√¢n t√≠ch s·∫£n ph·∫©m", "Ph√¢n t√≠ch d·ªØ li·ªáu m·ªõi"]
    choice = st.sidebar.selectbox("", menu)

    # Ti√™u ƒë·ªÅ Th√†nh vi√™n th·ª±c hi·ªán
    st.sidebar.markdown("""
    <div style="color: green; font-size: 16px; font-weight: bold; margin-top: 20px;">
        Th√†nh vi√™n th·ª±c hi·ªán:
    </div>
    """, unsafe_allow_html=True)

    # Hi·ªÉn th·ªã ·∫£nh Nguy·ªÖn Th·ªã M·ª∑ Ti√™n
    st.sidebar.image("media/tien.jpg", width=150, caption="Nguy·ªÖn Th·ªã M·ª∑ Ti√™n", use_container_width=False)

    # Hi·ªÉn th·ªã ·∫£nh ƒê·∫∑ng Th·ªã Th·∫£o
    st.sidebar.image("media/thao.jpg", width=150, caption="ƒê·∫∑ng Th·ªã Th·∫£o", use_container_width=False)
    
    # Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n
    st.sidebar.markdown("""
    <div style="color: green; font-size: 16px; font-weight: bold; margin-top: 20px;">
        Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n:
    </div>
    """, unsafe_allow_html=True)
    # Hi·ªÉn th·ªã ·∫£nh gi·∫£ng vi√™n
    st.sidebar.image("media/co_phuong.jpg", width=150, caption="C√¥ Khu·∫•t Thu·ª≥ Ph∆∞∆°ng", use_container_width=False)
    
    # Th·ªùi gian th·ª±c hi·ªán
    st.sidebar.markdown("""
    <div style="color: green; font-size: 16px; font-weight: bold; margin-top: 20px;">
        Th·ªùi gian th·ª±c hi·ªán:
    </div>
    <div style="border: 1px solid #ccc; padding: 10px; border-radius: 5px; background-color: #f9f9f9; margin-top: 5px;">
        16/12/2024
    </div>
    """, unsafe_allow_html=True) 

    if choice == 'M·ª•c ti√™u d·ª± √°n':
        business_objective_content()
    elif choice == 'Th·ª±c hi·ªán d·ª± √°n':
        build_project_construction()
    elif choice == 'Ph√¢n t√≠ch s·∫£n ph·∫©m':
        build_product_analysis()
    elif choice == 'Ph√¢n t√≠ch d·ªØ li·ªáu m·ªõi':
        new_product_analysis()